"""
This script has all CONFIGURATION SETTINGS TO RUN CRAWLER FROM main.py
Change configuration based on website & crawling strategy
"""
import os, csv, json, asyncio
from pydantic import BaseModel
from crawl4ai import BrowserConfig, CrawlerRunConfig, LLMConfig, LLMExtractionStrategy, JsonCssExtractionStrategy, CacheMode

## CONTROL VARIABLES
TEST_MODE = True
CRAWL_NUMBER = 0
PAGE_NUMBER = 1
DELAY_TIME = 5


MAIN_FILE= "D:/My Codes/Projects/Project-001/Database/Computer-village_products.csv"
TEST_FILE= "D:/My Codes/Projects/Project-001/Crawler/trials/test_csv.csv"
SESSION_ID = "project-001"



## STRATEGY
CSS_SELECTOR = ".card.h-100"
URLS_TO_CRAWL = [
        {"category": "laptop", "base_url": "https://www.ryans.com/category/laptop"}, #0
        {"category": "desktop_and_server", "base_url": "https://www.ryans.com/category/desktop-and-server"}, #1
        {"category": "gaming", "base_url": "https://www.ryans.com/category/gaming"}, #2
        {"category": "monitor", "base_url": "https://www.ryans.com/category/monitor"}, #3
        {"category": "tablet_pc", "base_url": "https://www.ryans.com/category/tablet"}, #4
        {"category": "printer", "base_url": "https://www.ryans.com/category/printer"}, #5
        {"category": "camera", "base_url": "https://www.ryans.com/category/camera"}, #6
        {"category": "security", "base_url": "https://www.ryans.com/category/security-system"}, #7
        {"category": "network", "base_url": "https://www.ryans.com/category/network"}, #8
        {"category": "sound", "base_url": "https://www.ryans.com/category/sound-system"}, #9
        {"category": "office_items", "base_url": "https://www.ryans.com/category/office-items"}, #10
        {"category": "accessories", "base_url": "https://www.ryans.com/category/accessories"}, #11
        {"category": "software", "base_url": "https://www.ryans.com/category/software"}, #12
        {"category": "gadget", "base_url": "https://www.ryans.com/category/gadget"}, #13
        {"category": "store", "base_url": "https://www.ryans.com/category/store"} #14
    ]

# Handcrafted schema for JsonCssExtractionStrategy by inspecting the webpage
SCHEMA_FOR_EXTRACTION = {
        "name": "Product",
        "baseSelector": ".card.h-100",            
        "fields": [
            {"name": "name", "selector": ".card-text.p-0.m-0.list-view-text a", "type": "attribute", "attribute": "data-bs-original-title"},
            {"name": "image_url", "selector": ".image-box img", "type": "attribute", "attribute": "src"},
            {"name": "description", "selector": ".short-desc-attr li", "type": "list", "fields": [{"name": "feature","type": "text"}]},
            {"name": "price", "selector": ".pr-text.cat-sp-text.pb-1.text-dark.text-decoration-none", "type": "text"},
            {"name": "url", "selector": ".card-text.p-0.m-0.list-view-text a", "type": "attribute", "attribute": "href"}
            ] 
        }
CATEGORY = URLS_TO_CRAWL[CRAWL_NUMBER]['category']
URL = f"{URLS_TO_CRAWL[CRAWL_NUMBER]['base_url']}?page={PAGE_NUMBER}" 


# PYDANTIC SCHEMA FOR LLM-BASED EXTRACTION STRATEGY
class Products(BaseModel):
    category : str
    name : str
    image_url : str
    description : str
    price : int
    url : str


## CONFIGURATION SETTINGS
def get_browser_config():
    return BrowserConfig(
        browser_type='chromium', # Chrome Browser
        headless=False, # Headless == No GUI
        verbose=True # Verbose logging
    ) 


# CRAWL THE URL
async def crawl_and_extract_from_page(crawler):
    result = await crawler.arun(
        url = URL,
        config=CrawlerRunConfig(
            css_selector=CSS_SELECTOR,
            extraction_strategy=JsonCssExtractionStrategy(SCHEMA_FOR_EXTRACTION),
            session_id=SESSION_ID,
            cache_mode=CacheMode.BYPASS
        )
    )
    
    print(result.markdown)
    print(type(result.extracted_content))
    return result

# OUTPUT
product_count = 0
crawled_page_count = 0
seen_product = ()
async def output_pipeline(result):
    extracted_data = json.loads(result.extracted_content)
    # Stop if no products found
    if not extracted_data:
        print(f"No products found in Page {PAGE_NUMBER}.")
        return 
                
    # Append new unique products
    new_products = [] 
    for item in extracted_data:
        new_products.append(item)
        print(f"Update: Page {PAGE_NUMBER}: Extracted {len(extracted_data)} products.")
                
        # Add products in CSV file
        if TEST_MODE:
            custom_csv_writer(
                file_to_be_written=TEST_FILE,
                products=new_products
                )
            product_count = product_count + len(new_products)
            crawled_page_count += 1
            print(f"Update: Page {PAGE_NUMBER}: Written {len(new_products)} new products to database. Total written: {product_count}")
            print(f"TEST MODE SUCCESSFUL. WAIT FOR FINAL LOG")
            return
                
        else:
            custom_csv_writer(
                file_to_be_written=MAIN_FILE,
                products=new_products
                )
            # Count of total products extracted & written
            product_count = product_count + len(new_products)
            print(f"Update: Page {PAGE_NUMBER}: Written {len(new_products)} new products to database. Total written: {product_count}")
                
            # Proceed to next page
            PAGE_NUMBER += 1
            crawled_page_count += 1
            # Delay Log
            print(f"Proceeding to next page after {DELAY_TIME} seconds...")
            await asyncio.sleep(DELAY_TIME) # Be polite and avoid overwhelming the server


## WRITING
HEADERS = ['category'] + [field['name'] for field in SCHEMA_FOR_EXTRACTION['fields']]

def custom_csv_writer(file_to_be_written, products):    
    file_exists = os.path.isfile(file_to_be_written)
    with open(file_to_be_written, mode='a', newline='', encoding='utf-8') as file:
        writer = csv.writer(file)

        # Write header only if file is new, otherwise assume, there is already a header
        if not file_exists:
            writer.writerow(HEADERS)
        
        # THIS SECTION IS EXTRACTION SPECIFIC::: Write as per extracted data
        for product in products:
            product['category'] = CATEGORY
            price = product.get("price", "")
            if price and price.startswith("Ex Tax:"):
                product["price"] = price.replace("Ex Tax:", "").strip()

            row = [product.get('category', '')] + [product.get(field, '') for field in HEADERS[1:]]
            writer.writerow(row)